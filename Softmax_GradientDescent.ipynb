{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae096850-5dd1-42c4-90c8-50a347b73ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: cost = 1.1123483200063804\n",
      "Step 1000: cost = 0.15031261345548627\n",
      "Step 2000: cost = 0.08015340510930193\n",
      "Step 3000: cost = 0.05331286763848229\n",
      "Step 4000: cost = 0.03959979558876129\n",
      "Step 5000: cost = 0.031379339121557104\n",
      "Step 6000: cost = 0.025934384769552735\n",
      "Step 7000: cost = 0.022074762092810105\n",
      "Step 8000: cost = 0.019201658213319397\n",
      "Step 9000: cost = 0.016982474517266546\n",
      "Final weights: [[-0.77468941 -0.23098022  1.01573101]\n",
      " [ 0.33582767  1.22953153 -1.55481181]]\n",
      "Final bias: [ 3.2648068  -2.59633404 -0.66847277]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z))  # To prevent overflow\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "# Loss function for all samples\n",
    "def loss(w, b, x, y):\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "    \n",
    "    # Loop over each sample\n",
    "    for i in range(m):\n",
    "        z = np.dot(x[i], w) + b  # Linear transformation\n",
    "        y_pred = softmax(z)  # Softmax output\n",
    "        # Cross-entropy loss for the current sample\n",
    "        cost -= np.sum(y[i] * np.log(y_pred))  # Only consider the correct class\n",
    "    \n",
    "    return cost / m  # Average cost over all samples\n",
    "\n",
    "# Compute gradient descent\n",
    "def gradient_descent(w, b, x, y, alpha, max_steps=10000):\n",
    "    m, n = x.shape  # m = number of samples, n = number of features\n",
    "    for step in range(max_steps):\n",
    "        # Compute the loss (for debugging purposes)\n",
    "        cost = loss(w, b, x, y)\n",
    "        \n",
    "        # Gradients initialization\n",
    "        dw = np.zeros_like(w)\n",
    "        db = np.zeros_like(b)\n",
    "        \n",
    "        # Compute gradients\n",
    "        for i in range(m):\n",
    "            z = np.dot(x[i], w) + b\n",
    "            y_pred = softmax(z)\n",
    "            \n",
    "            # Gradient for weights: (y_pred - y_true) * x\n",
    "            dw += np.outer(x[i], (y_pred - y[i]))\n",
    "            # Gradient for bias: (y_pred - y_true)\n",
    "            db += (y_pred - y[i])\n",
    "        \n",
    "        # Average gradients over all samples\n",
    "        dw /= m\n",
    "        db /= m\n",
    "        \n",
    "        # Update weights and bias\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        \n",
    "        # Debugging: Print cost every 1000 steps\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step}: cost = {cost}\")\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Random initialization\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W = np.random.randn(2, 3) * 0.01  # 2 features, 3 classes\n",
    "b = np.zeros(3)  # 3 classes, so 1 bias per class\n",
    "\n",
    "# Sample input data\n",
    "x = np.array([[1.0, 2.0], [1.5, 1.8], [5.8, 8.0], [6.0, 9.0], [8.0, 1.0], [9.0, 2.5]])\n",
    "y = np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1]])\n",
    "\n",
    "# Run gradient descent\n",
    "w, b = gradient_descent(W, b, x, y, alpha=0.01)\n",
    "\n",
    "print(\"Final weights:\", w)\n",
    "print(\"Final bias:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2ff2f5-c966-43ab-8454-d9aa717d858b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3+6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7883dcc7-be22-4ccc-962f-63b49bd05d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d0343f-ebe9-4461-9d49-4d3eec6a743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a6251-dd6f-43bd-86c9-f88095241cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z=np.exp(z-np.max(z))\n",
    "    return exp_z/np.sum(exp_z)\n",
    "\n",
    "def loss(w,b,x,y):\n",
    "    m=x.shape[0]\n",
    "    cost=0\n",
    "    for i in range(m):\n",
    "        z=np.dot(x[i],w)+b\n",
    "        y_pred=softmax(z)\n",
    "        cost -= np.sum(y[i]*np.log(y_pred))\n",
    "    return cost/m\n",
    "\n",
    "def gradient_descent(w,b,x,y,init_cost,alpha,max_step=10000,step=0):\n",
    "    m=x.shape[0]\n",
    "    n=len(w)\n",
    "    if init_cost is None:\n",
    "        init_cost=loss(w,b,x,y)\n",
    "    new_w=w.copy()\n",
    "    new_b=b.copy()\n",
    "    dw = np.zeros_like(w)\n",
    "    db = np.zeros_like(b)\n",
    "    for i in range(m):\n",
    "        z = np.dot(x[i], new_w) + new_b\n",
    "        y_pred = softmax(z)\n",
    "        # Gradient for weights: (y_pred - y_true) * x\n",
    "        dw += np.outer(x[i], (y_pred - y[i]))\n",
    "        # Gradient for bias: (y_pred - y_true)\n",
    "        db += (y_pred - y[i])\n",
    "    dw/=m\n",
    "    db/=m\n",
    "    new_w-=alpha*dw\n",
    "    new_b-=alpha*db\n",
    "    new_cost=loss(new_w,new_b,x,y)\n",
    "    if new_cost<init_cost and step<max_step:\n",
    "        return gradient_descent(new_w,new_b,x,y,new_cost,alpha,max_step,step+1)\n",
    "    else:\n",
    "        return new_w,new_b\n",
    "\n",
    "np.random.seed(42)  # for consistent results\n",
    "W = np.random.randn(2, 3) * 0.01  # (2 features, 3 classes)\n",
    "b = np.zeros((1, 3)) \n",
    "\n",
    "x=np.array([[1.0,2.0],[1.5,1.8],[5.8,8.0],[6.0,9.0],[8.0,1.0],[9.0,2.5]])\n",
    "\n",
    "y=np.array([[1,0,0],[1,0,0],[0,1,0],[0,1,0],[0,0,1],[0,0,1]])\n",
    "\n",
    "gradient_descent(W,b,x,y,None,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209047b4-1256-47f6-bf49-e8be546feae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
